% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage[]{ACL2023}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Project Proposal CSCI 544: Attenuating Bias in Pre-trained Language Models using MABEL}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}

\author{{\bf Aditi Bodhankar}, {\bf Aditya Mantri Anulekh}, {\bf Advait Rane}, {\bf Nithyashree Manohar}, {\bf Surya Teja CVN}}

\begin{document}
\maketitle
% \begin{abstract}
% \end{abstract}

\section{Domain and Goals}
Pre-trained deep learning models encode harmful social biases which can have unintended consequences in downstream tasks. Language models inherit social biases when they are trained on text corpora that contain examples of such biases. Large Language Models have been to shown to encode biases along gender and religious lines. This has prompted recent research to explore ways to debias models and produce fair model outcomes. For our project, we aim to implement once such strategy, MABEL \cite{he2022mabel}, to reduce biases in pre-trained language models through a task-agnostic pre-training step.

MABEL describes an intermediate pre-training step to reduce gender bias in language representations. They use Natural Language Inference (NLI) as a pre-training task and augment the dataset to contain samples of the same sentence but with opposite genders. For example, if the sentence, "Woman putting together a wooden shelf" is present in the dataset, they add a new sentence, "Man putting together a wooden shelf". The paper restricts itself to the binary gender. 

The model is trained on the NLI task along with an alignment loss, to minimize the distance between original sentences and their augmented counterparts, and a contrastive loss, based on entailment of inference pairs. They then evaluate the model using intrinsic and extrinsic fairness metrics.

The goals for this project are-
\begin{enumerate}
    \item Implementing and evaluating MABEL, as described in the original paper.
    \item Evaluating the use of MABEL to reduce social biases along other dimensions, e.g., racial or religious
    \item Evaluating alternative strategies to debias model using the MABEL framework, e.g., by pre-training on a different task.
\end{enumerate}

\section{Related Work}

\subsection{Bias in NLP} 

Bias in Natural Language Processing is a pressing issue. Deep learning models are susceptible to bias because they are trained on large amounts of data that may contain implicit biases which leads the model to behave negatively towards unrepresented groups. Many deep learning models exhibit occupational gender stereotypes \cite{sun-etal-2019-mitigating}. Models predict “He is doctor” with a higher likelihood than “She is doctor.” Word embeddings encode relationships such that `man' is to `woman' as `computer programmer' is to `homemaker'. Translating “He is a nurse. She is a doctor” into Hungarian and back to English results in “She is a nurse. He is a doctor.” 

There are other forms of bias prominent in deep learning models. Embeddings for `Black' is to `criminal' as `Caucasian' is to police. Similarly, `lawful' is to `Christianity' as `terrorist' is to `Islamic'. AI models are more likely to flag tweets written by African Americans as offensive \cite{manzini-etal-2019-black}. Research shows that multiple gender stereotypes occur in GPT-3 generated narratives, and can emerge even when prompts do not contain explicit gender cues or stereotype-related content \cite{lucy-bamman-2021-gender}.


\subsection{Mitigating Bias in NLP}

Mitigating bias in NLP can be divided into two broad categories - task-specific and task-agnostic debiasing \cite{he2022mabel}. In the first category, the model learns to discard the influence of sensitive attributes during downstream tasks. In the second category, the model mitigates bias by leveraging textual information from general corpora. For example, mitigating gender bias in NLP models, can involve computing a gender subspace representation and eliminating it from the encoded representations \cite{https://doi.org/10.48550/arxiv.1607.06520}. Some of the other methods involve re-training the encoder with higher dropout or equalizing objectives. In this work, we propose to explore task-agnostic methods to mitigate gender bias \cite{gira-etal-2022-debiasing}.

\subsection{Evaluating Bias in NLP}





\section{Datasets}

We intend to follow the MABEL paper where the experiments are performed on two renowned NLI datasets, which are, Stanford Natural Language Inference (SNLI) \cite{SNLI_dataset_paper} and the Multi-Genre Natural Language Inference (MNLI) \cite{MNLI_dataset_paper}. The MNLI dataset is modeled on the SNLI corpus, which gives a range of genres of spoken and written text, and supports a distinctive cross-genre generalization evaluation. The two datasets are available at - \url{https://nlp.stanford.edu/projects/snli/} and \url{https://cims.nyu.edu/~sbowman/multinli/} respectively.
We extract the sentence pairs with an entailment relationship as a pre-processing step. This basically is a hypothesis sentence that can be inferred to be true, based on a premise sentence. Since gender attribute is the prime focus, we extract all entailment pairs that contain at least one gendered term in either the premise or the hypothesis from an NLI dataset. For a sensitive attribute term in the sequence, swapping of a word is done along the opposite bias direction, that is, by changing "girl" to "boy" and the non-attribute words remain the same. This approach is followed for each sentence in every entailment pair. We also intend to perform few data cleaning activities that are suitable for the application.

\section{Technical Challenge}



\bibliography{custom}
\bibliographystyle{acl_natbib}

\appendix

\end{document}
