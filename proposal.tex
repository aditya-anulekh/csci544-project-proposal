% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage[]{ACL2023}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Project Proposal CSCI 544: Attenuating Bias in Pre-trained Language Models using MABEL}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}

\author{{\bf Aditi Bodhankar} \\ {\bf Aditya Anulekh Mantri} \\ {\bf Advait Rane} \\ {\bf Nithyashree Manohar} \\ {\bf Surya Teja CVN} \\ \{bodhanka, adityaan, aprane, nithyash, suryatej\}@usc.edu} 



\begin{document}
\maketitle
% \begin{abstract}
% \end{abstract}

\section{Domain and Goals}
Pre-trained deep learning models encode harmful social biases, which can have unintended consequences in downstream tasks. Language models
inherit social biases when trained on text
corpora containing examples of such biases. Large Language Models have been shown to encode biases along gender and religious lines. This has prompted recent research to explore ways to de-bias models and produce fair model outcomes. For our project, we aim to implement one such strategy, MABEL \cite{he2022mabel}, to reduce biases in pre-trained language models through a pre-training step.

MABEL describes an intermediate pre-training step to reduce gender bias in language representations. They use Natural Language Inference (NLI) as a pre-training task and augment the NLI dataset to include samples of the same sentence but with opposite genders. For example, if the sentence, "Woman putting together a wooden shelf." is present in the dataset, they add a new sentence, "Man putting together a wooden shelf". The paper restricts itself to the binary gender. 

The model is trained on the NLI task along with an alignment loss to minimize the distance between original sentences and their augmented counterparts, and a contrastive loss based on entailment of inference pairs. They then evaluate the model using intrinsic and extrinsic fairness metrics. 

The goals for this project are-
\begin{enumerate}
    \item Implementing and evaluating MABEL to reduce gender bias in language models, as described in the original paper.
    \item Evaluating the use of MABEL to reduce social biases along other dimensions, e.g., racial or religious
    \item Evaluating alternative strategies to de-bias models using MABEL-like augmented datasets e.g. by pre-training on a different task.
\end{enumerate}

\section{Related Work}

\subsection{Bias in NLP} 

Bias in Natural Language Processing is a pressing issue. Deep learning models are susceptible to bias because they are trained on large amounts of data that may contain implicit biases, leading the model to behave negatively towards unrepresented groups. Many deep learning models exhibit occupational gender stereotypes \cite{sun-etal-2019-mitigating}. Models predict “He is a doctor” with a higher likelihood than “She is a doctor.” Word embeddings encode relationships such that `man' is to `woman' as `computer programmer' is to `homemaker'. Translating “He is a nurse. She is a doctor” into Hungarian and back to English results in “She is a nurse. He is a doctor.” 

There are other forms of bias prominent in deep learning models. Embeddings for `Black' is to `criminal' as `Caucasian' is to police. Similarly, `lawful' is to `Christianity' as `terrorist' is to `Islamic'. AI models are more likely to flag tweets written by African Americans as offensive \cite{manzini-etal-2019-black}. Research shows that multiple gender stereotypes occur in GPT-3 generated narratives, and can emerge even when prompts do not contain explicit gender cues or stereotype-related content \cite{lucy-bamman-2021-gender}.


\subsection{Mitigating Bias in NLP}

Mitigating bias in NLP can be divided into two broad categories - task-specific and task-agnostic debiasing \cite{he2022mabel}. In the first category, the model learns to discard the influence of sensitive attributes during downstream tasks. In the second category, the model mitigates bias by leveraging textual information from general corpora. For example, mitigating gender bias in NLP models can involve computing a gender subspace representation and eliminating it from the encoded representations \cite{https://doi.org/10.48550/arxiv.1607.06520}. Other methods involve re-training the encoder with higher dropout or equalizing objectives. In this work, we propose to explore task-agnostic methods to mitigate gender bias \cite{gira-etal-2022-debiasing}.

\subsection{Evaluating Bias in NLP}

Evaluating Bias in NLP models is as critical as identifying and mitigating bias. There are two types of bias evaluation - \textit{intrinsic} and \textit{extrinsic} \cite{he2022mabel}. Intrinsic evaluation methods directly probe the language model by either measuring the geometry of the embedding space or through likelihood scoring. Extrinsic methods, on the other hand, evaluate bias in models by using the model's predictions for a large population of downstream tasks. 

Although intrinsic evaluation metrics are opaque they are popular in contemporary works due to their ease of computation. Compared to intrinsic methods, extrinsic methods are much more compute-intensive and time-consuming. However, extrinsic methods are interpretable and are known to better identify bias in language models and flag social harm. 

\section{Datasets}

We intend to follow the MABEL paper, where the experiments are performed on two renowned NLI datasets, which are Stanford Natural Language Inference (SNLI) \cite{SNLI_dataset_paper} and the Multi-Genre Natural Language Inference (MNLI) \cite{MNLI_dataset_paper}. The MNLI dataset \footnote{\url{https://cims.nyu.edu/~sbowman/multinli/}} is modelled on the SNLI corpus \footnote{\url{https://nlp.stanford.edu/projects/snli/}}, which gives a range of genres of spoken and written text and supports a distinctive cross-genre generalization evaluation. 

We extract the sentence pairs with an entailment relationship as a pre-processing step. This includes a hypothesis sentence that can be inferred to be true based on a premise sentence. Since gender attribute is the prime focus, we extract all entailment pairs that contain at least one gendered term in either the premise or the hypothesis from an NLI dataset. For a sensitive attribute term in the sequence, we swap the word along the opposite bias direction, i.e., by changing "girl" to "boy", while the non-attribute words remain the same. This approach is followed for each sentence in every entailment pair. We also intend to perform a few suitable data-cleaning activities for the application.

\section{Technical Challenge}
One of our main challenges is encoding social biases that are easily compounded in downstream tasks. Evaluating bias can be a very dynamic area of recent research and will need further improvement before it can be turned into a measurable outcome. Additionally, we perceive that some parts of this project are reductive, for instance, binary genders. We do not intend to enforce such reductive discourses, and maybe try to expand to a comprehensive solution to the best of our efforts. As a part of our project, we do not intend to find a complete solution to fix biases in models but will proceed to study ways to reduce these biased outcomes. 

As for computational needs, the paper employs 4x NVIDIA GeForce RTX 3090 GPUs for implementing MABEL. We will be downgrading this, which can increase the training and inference time. The paper also highlights a concern for the learning rate, $\alpha$, where our goal will be to optimize the learning rate to balance the trade-off between the fairer stereotype score and the language modeling ability. 

\section{Individual Contributions}

\begin{table}[h]
\begin{tabular}{|c|c|}
\hline
\textbf{Task}                & \textbf{Assignee}                            \\ \hline
Report/Proposal     & Everyone                              \\ \hline
Data Preprocessing  & Surya Teja \& Aditi     \\ \hline
Model Coding        & Advait and Aditya Anulekh \\ \hline
Evaluation Metrics  & Nithyashree                   \\ \hline
Analysis of Results & Everyone                              \\ \hline
\end{tabular}
\end{table}


\bibliography{custom}
\bibliographystyle{acl_natbib}

\appendix

\end{document}
