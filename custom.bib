% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@article{he2022mabel,
  title={MABEL: Attenuating Gender Bias using Textual Entailment Data},
  author={He, Jacqueline and Xia, Mengzhou and Fellbaum, Christiane and Chen, Danqi},
  journal={arXiv preprint arXiv:2210.14975},
  year={2022}
}

@article{SNLI_dataset_paper,
  title={A large annotated corpus for learning natural language inference},
  author={Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning},
  journal={In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 632–642, Lisbon, Portugal. Association for Computational Linguistics},
  year={2015}
}

@article{MNLI_dataset_paper,
  title={A broad-coverage challenge corpus for sentence understanding through inference},
  author={Adina Williams, Nikita Nangia, and Samuel Bowman},
  journal={In Proceedings of the 2018 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112–1122, New Orleans, Louisiana. Association for Computational Linguistics},
  year={2018}
}

@inproceedings{sun-etal-2019-mitigating,
    title = "Mitigating Gender Bias in Natural Language Processing: Literature Review",
    author = "Sun, Tony  and
      Gaut, Andrew  and
      Tang, Shirlyn  and
      Huang, Yuxin  and
      ElSherief, Mai  and
      Zhao, Jieyu  and
      Mirza, Diba  and
      Belding, Elizabeth  and
      Chang, Kai-Wei  and
      Wang, William Yang",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1159",
    doi = "10.18653/v1/P19-1159",
    pages = "1630--1640",
}

@inproceedings{manzini-etal-2019-black,
    title = "Black is to Criminal as Caucasian is to Police: Detecting and Removing Multiclass Bias in Word Embeddings",
    author = "Manzini, Thomas  and
      Yao Chong, Lim  and
      Black, Alan W  and
      Tsvetkov, Yulia",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1062",
    doi = "10.18653/v1/N19-1062",
    pages = "615--621",
}

@inproceedings{lucy-bamman-2021-gender,
    title = "Gender and Representation Bias in {GPT}-3 Generated Stories",
    author = "Lucy, Li  and
      Bamman, David",
    booktitle = "Proceedings of the Third Workshop on Narrative Understanding",
    month = jun,
    year = "2021",
    address = "Virtual",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.nuse-1.5",
    doi = "10.18653/v1/2021.nuse-1.5",
    pages = "48--55",
}

@inproceedings{gira-etal-2022-debiasing,
    title = "Debiasing Pre-Trained Language Models via Efficient Fine-Tuning",
    author = "Gira, Michael  and
      Zhang, Ruisu  and
      Lee, Kangwook",
    booktitle = "Proceedings of the Second Workshop on Language Technology for Equality, Diversity and Inclusion",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.ltedi-1.8",
    doi = "10.18653/v1/2022.ltedi-1.8",
    pages = "59--69",
    abstract = "An explosion in the popularity of transformer-based language models (such as GPT-3, BERT, RoBERTa, and ALBERT) has opened the doors to new machine learning applications involving language modeling, text generation, and more. However, recent scrutiny reveals that these language models contain inherent biases towards certain demographics reflected in their training data. While research has tried mitigating this problem, existing approaches either fail to remove the bias completely, degrade performance ({``}catastrophic forgetting{''}), or are costly to execute. This work examines how to reduce gender bias in a GPT-2 language model by fine-tuning less than 1{\%} of its parameters. Through quantitative benchmarks, we show that this is a viable way to reduce prejudice in pre-trained language models while remaining cost-effective at scale.",
}

@misc{https://doi.org/10.48550/arxiv.1607.06520,
  doi = {10.48550/ARXIV.1607.06520},
  
  url = {https://arxiv.org/abs/1607.06520},
  
  author = {Bolukbasi, Tolga and Chang, Kai-Wei and Zou, James and Saligrama, Venkatesh and Kalai, Adam},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings},
  
  publisher = {arXiv},
  
  year = {2016},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

